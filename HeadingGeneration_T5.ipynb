{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HeadingGeneration-T5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkxKVQoZTSvDq94nXB24de",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee50b9256d3e4f3d9035a2f3d9c4124d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e394027a560e46cab2bc5c7b359adfd3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7931171a1dff427db6fe398e42f7852c",
              "IPY_MODEL_96abe6c2755f40a591c100293f0ee62b"
            ]
          }
        },
        "e394027a560e46cab2bc5c7b359adfd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7931171a1dff427db6fe398e42f7852c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_d0535d54c7d04166a9058a4881ea4fe2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.98MB of 0.98MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1386a7e857134e969691933e0dfa021d"
          }
        },
        "96abe6c2755f40a591c100293f0ee62b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6717b1054edc456e9766a4229ae3fc50",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_969bc7c2239948649d53a2ac52e32f1c"
          }
        },
        "d0535d54c7d04166a9058a4881ea4fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1386a7e857134e969691933e0dfa021d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6717b1054edc456e9766a4229ae3fc50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "969bc7c2239948649d53a2ac52e32f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sehgalsakshi/Text-Summarization-and-Headline-Generation-Using-T5/blob/main/HeadingGeneration_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tstAJFlt8L2"
      },
      "source": [
        "#Fine Tuned T5 for Abstractive Headlines Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DH0ZJ97cyz6"
      },
      "source": [
        "T5 is a text to text transformer where encoder recieves a sequence and also decoder outputs a sequence.\r\n",
        "But here all tasks are modelled in the same way unlike BERT.\r\n",
        "For example, Bert has to be fine tuned differently for different tasks but in T5, fine tuning is same for all the tasks, just the task name has to be mentioned in input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2mPyj1xdibO"
      },
      "source": [
        "**Why T5 for Summarization?**\r\n",
        "\r\n",
        "T5 performs abstractive summarization in contrast to extractive. \r\n",
        "\r\n",
        "**Extractive summarization** means **identifying important sections** of the text and generating them verbatim producing a subset of the sentences from the original text while **Abstractive summarization** **reproduces important material in a new way** after interpretation and examination of the text using advanced natural language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBE9ZQ_9efeC"
      },
      "source": [
        "T5 model has been trained on a very large dataset thus it's pretrained model is sufficient to perform generic text summarization. But if you've a domain specific dataset, one can consider fine tuning it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ndcv9jafTA1"
      },
      "source": [
        "Here we're performing **Abstractive Headlines Generation** using T5 summarization task. \r\n",
        "Since T5 generates summarization, we're fine tuning it to perform summarization for restricted number of words, thus getting a heading for the text.\r\n",
        "\r\n",
        "Input would be a Text column and target to train for would be Headlines column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2VebLVdtHb1",
        "outputId": "2529db3d-66ca-4997-dd3e-d040a7f73618"
      },
      "source": [
        "#install the requirements\r\n",
        "!pip install transformers -q\r\n",
        "!pip install wandb -q\r\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFmqkmaptPQ9"
      },
      "source": [
        "# Importing libraries\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "import sentencepiece\r\n",
        "\r\n",
        "# Importing the T5 modules from huggingface/transformers\r\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\r\n",
        "\r\n",
        "# WandB – Import the wandb library to log the model run and all the parameters\r\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTGWXaHYuK3L",
        "outputId": "907c251a-563e-440f-aac0-31613de68876"
      },
      "source": [
        "# Checking the GPU we have access to.\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Dec 27 21:35:42 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "820vjNwMuQXS"
      },
      "source": [
        "# Setting up the device for GPU usage\r\n",
        "from torch import cuda\r\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwUWybvTuXoT",
        "outputId": "b89787ed-5efe-4409-a952-f758457462a6"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msakshisehgal\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hah5C1Gdu5tG"
      },
      "source": [
        "'''Creating a custom dataset for reading the dataset and \r\n",
        "loading it into the dataloader to pass it to the neural network for finetuning the model'''\r\n",
        "\r\n",
        "class HeadlinesDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.data = dataframe\r\n",
        "        self.source_len = source_len\r\n",
        "        self.summ_len = summ_len\r\n",
        "        self.headlines = self.data.headlines\r\n",
        "        self.ctext = self.data.ctext\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.headlines)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        ctext = str(self.ctext[index])\r\n",
        "        headlines = str(self.headlines[index])\r\n",
        "\r\n",
        "        #cleaning data so as to ensure data is in string type\r\n",
        "        ctext = ' '.join(ctext.split())\r\n",
        "        headlines = ' '.join(headlines.split())\r\n",
        "\r\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\r\n",
        "        target = self.tokenizer.batch_encode_plus([headlines], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\r\n",
        "\r\n",
        "        source_ids = source['input_ids'].squeeze()\r\n",
        "        source_mask = source['attention_mask'].squeeze()\r\n",
        "        target_ids = target['input_ids'].squeeze()\r\n",
        "        target_mask = target['attention_mask'].squeeze()\r\n",
        "\r\n",
        "        return {\r\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \r\n",
        "            'source_mask': source_mask.to(dtype=torch.long), \r\n",
        "            'target_ids': target_ids.to(dtype=torch.long),\r\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\r\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_T1LXdkvUvW"
      },
      "source": [
        "#Function to be called for training with the parameters passed from main function\r\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\r\n",
        "    model.train()\r\n",
        "    for _,data in enumerate(loader, 0):\r\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\r\n",
        "        y_ids = y[:, :-1].contiguous()\r\n",
        "        lm_labels = y[:, 1:].clone().detach()\r\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\r\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\r\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\r\n",
        "\r\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\r\n",
        "        loss = outputs[0]\r\n",
        "        \r\n",
        "        if _%10 == 0:\r\n",
        "            wandb.log({\"Training Loss\": loss.item()})\r\n",
        "\r\n",
        "        if _%500==0:\r\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3q6onLjvdF5"
      },
      "source": [
        "#Function to evaluate model for predictions\r\n",
        "def validate(epoch, tokenizer, model, device, loader):\r\n",
        "    model.eval()\r\n",
        "    predictions = []\r\n",
        "    actuals = []\r\n",
        "    with torch.no_grad():\r\n",
        "        for _, data in enumerate(loader, 0):\r\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\r\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\r\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\r\n",
        "\r\n",
        "            generated_ids = model.generate(\r\n",
        "                input_ids = ids,\r\n",
        "                attention_mask = mask, \r\n",
        "                max_length=150, \r\n",
        "                num_beams=2,\r\n",
        "                repetition_penalty=2.5, \r\n",
        "                length_penalty=1.0, \r\n",
        "                early_stopping=True\r\n",
        "                )\r\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\r\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\r\n",
        "            if _%100==0:\r\n",
        "                print(f'Completed {_}')\r\n",
        "\r\n",
        "            predictions.extend(preds)\r\n",
        "            actuals.extend(target)\r\n",
        "    return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdBn7nTzkSGB"
      },
      "source": [
        "model_dir = 'model'\r\n",
        "os.mkdir(model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ee50b9256d3e4f3d9035a2f3d9c4124d",
            "e394027a560e46cab2bc5c7b359adfd3",
            "7931171a1dff427db6fe398e42f7852c",
            "96abe6c2755f40a591c100293f0ee62b",
            "d0535d54c7d04166a9058a4881ea4fe2",
            "1386a7e857134e969691933e0dfa021d",
            "6717b1054edc456e9766a4229ae3fc50",
            "969bc7c2239948649d53a2ac52e32f1c"
          ]
        },
        "id": "5dNQWGj4vgQf",
        "outputId": "1700a3b1-0af9-4ffe-ed1b-56975c74c789"
      },
      "source": [
        "def main():\r\n",
        "    # Intialize new run in WandB\r\n",
        "    wandb.init(project=\"t5_headlines_summarization\")\r\n",
        "\r\n",
        "    # WandB – Config is a variable that holds and saves hyperparameters and inputs\r\n",
        "    # Defining some key variables that will be used later on in the training  \r\n",
        "    config = wandb.config          # Initialize config\r\n",
        "    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\r\n",
        "    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\r\n",
        "    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\r\n",
        "    config.VAL_EPOCHS = 1 \r\n",
        "    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\r\n",
        "    config.SEED = 42               # random seed (default: 42)\r\n",
        "    config.MAX_LEN = 512\r\n",
        "    config.SUMMARY_LEN = 20        #Generally this value is around 150 but since headlines are not that long, we're giving a realistic max word length \r\n",
        "\r\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\r\n",
        "    torch.manual_seed(config.SEED) # pytorch random seed\r\n",
        "    np.random.seed(config.SEED) # numpy random seed\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "    # tokenzier for encoding the text\r\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\r\n",
        "    \r\n",
        "\r\n",
        "    # Importing the raw dataset\r\n",
        "    # Since it's a sequence generation task, we can not perform data cleaning \r\n",
        "    #or else the output sequence would not be grammatically correct \r\n",
        "    df = pd.read_csv('news_summary.csv',encoding='utf-8')\r\n",
        "    print(df.columns)\r\n",
        "    #Using just the required columns\r\n",
        "    df = df[['headlines','ctext']]\r\n",
        "    df.ctext = 'summarize: ' + df.ctext\r\n",
        "    print(df.head())\r\n",
        "\r\n",
        "    \r\n",
        "    # Creation of Dataset and Dataloader\r\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest for validation. \r\n",
        "    train_size = 0.8\r\n",
        "    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\r\n",
        "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\r\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\r\n",
        "\r\n",
        "    print(\"FULL Dataset: {}\".format(df.shape))\r\n",
        "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\r\n",
        "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\r\n",
        "\r\n",
        "\r\n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\r\n",
        "    training_set = HeadlinesDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\r\n",
        "    val_set = HeadlinesDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\r\n",
        "\r\n",
        "    # Defining the parameters for creation of dataloaders\r\n",
        "    train_params = {\r\n",
        "        'batch_size': config.TRAIN_BATCH_SIZE,\r\n",
        "        'shuffle': True,\r\n",
        "        'num_workers': 0\r\n",
        "        }\r\n",
        "\r\n",
        "    val_params = {\r\n",
        "        'batch_size': config.VALID_BATCH_SIZE,\r\n",
        "        'shuffle': False,\r\n",
        "        'num_workers': 0\r\n",
        "        }\r\n",
        "\r\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\r\n",
        "    training_loader = DataLoader(training_set, **train_params)\r\n",
        "    val_loader = DataLoader(val_set, **val_params)\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \r\n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\r\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\r\n",
        "    model = model.to(device)\r\n",
        "\r\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \r\n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\r\n",
        "\r\n",
        "    # Log metrics with wandb\r\n",
        "    wandb.watch(model, log=\"all\")\r\n",
        "    # Training loop\r\n",
        "    print('Initiating Fine-Tuning for the model on our dataset')\r\n",
        "\r\n",
        "    for epoch in range(config.TRAIN_EPOCHS):\r\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\r\n",
        "    \r\n",
        "    #Saving the model state so that it can be reused for loading model in flask api\r\n",
        "    model_name = 'heading_model_cpu.pth' if device == 'cpu' else 'heading_model.pth'\r\n",
        "    path = './'+model_dir+'/'+model_name\r\n",
        "    torch.save(model, path)\r\n",
        "\r\n",
        "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\r\n",
        "    for epoch in range(config.VAL_EPOCHS):\r\n",
        "      predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\r\n",
        "      final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\r\n",
        "      final_df.to_csv('./'+model_dir+'/predictions.csv')\r\n",
        "    print('Output Files generated for review')    \r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:1rpt4fuh) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1341<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee50b9256d3e4f3d9035a2f3d9c4124d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.04MB of 0.04MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20201227_213543-1rpt4fuh/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20201227_213543-1rpt4fuh/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>Training Loss</td><td>1.14997</td></tr><tr><td>_step</td><td>361</td></tr><tr><td>_runtime</td><td>1431</td></tr><tr><td>_timestamp</td><td>1609106374</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>Training Loss</td><td>█▄▅▄▃▅▇▇▅▃▅▅▄▅▃▄▄▂▅▄▃▂▂▁▃▂▂▁▃▃▅▃▁▂▂▃▂▂▆▁</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">youthful-wildflower-10</strong>: <a href=\"https://wandb.ai/sakshisehgal/transformers_tutorials_summarization/runs/1rpt4fuh\" target=\"_blank\">https://wandb.ai/sakshisehgal/transformers_tutorials_summarization/runs/1rpt4fuh</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:1rpt4fuh). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.12<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">exalted-blaze-11</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sakshisehgal/transformers_tutorials_summarization\" target=\"_blank\">https://wandb.ai/sakshisehgal/transformers_tutorials_summarization</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/sakshisehgal/transformers_tutorials_summarization/runs/3diaef51\" target=\"_blank\">https://wandb.ai/sakshisehgal/transformers_tutorials_summarization/runs/3diaef51</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20201227_220601-3diaef51</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Index(['author', 'date', 'text', 'read_more', 'ctext'], dtype='object')\n",
            "                                                text                                              ctext\n",
            "0  Daman & Diu revokes mandatory Rakshabandhan in...  summarize: The Daman and Diu administration on...\n",
            "1  Malaika slams user who trolled her for 'divorc...  summarize: From her special numbers to TV?appe...\n",
            "2  'Virgin' now corrected to 'Unmarried' in IGIMS...  summarize: The Indira Gandhi Institute of Medi...\n",
            "3  Aaj aapne pakad liya: LeT man Dujana before be...  summarize: Lashkar-e-Taiba's Kashmir commander...\n",
            "4  Hotel staff to get training to spot signs of s...  summarize: Hotels in Mumbai and other Indian c...\n",
            "FULL Dataset: (4514, 2)\n",
            "TRAIN Dataset: (3611, 2)\n",
            "TEST Dataset: (903, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
            "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initiating Fine-Tuning for the model on our dataset\n",
            "Epoch: 0, Loss:  8.10882568359375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  1.833003044128418\n",
            "Epoch: 0, Loss:  2.382096767425537\n",
            "Epoch: 0, Loss:  2.4758236408233643\n",
            "Epoch: 1, Loss:  1.5662261247634888\n",
            "Epoch: 1, Loss:  1.3060849905014038\n",
            "Epoch: 1, Loss:  0.8978652358055115\n",
            "Epoch: 1, Loss:  1.0221585035324097\n",
            "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "Completed 300\n",
            "Completed 400\n",
            "Output Files generated for review\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}